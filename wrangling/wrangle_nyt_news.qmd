---
title: "Wrangling NBC News Articles"
subtitle: "STAT 231: Blog App"
author: "Maigan Lafontant, Emilie Ward, and Erika Salvador"
format: 
  pdf
header-includes: |
  \usepackage{fvextra}
  \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
---

```{r}
#| label: setup
#| include: false

# set code chunk option defaults
knitr::opts_chunk$set(
  # display code as types
  tidy = FALSE, 
  # slightly smaller code font
  size = "small",
  # do not display messages in PDF
  message = FALSE,
# improve digit and NA display 
  options(scipen = 1, knitr.kable.NA = ''))

# Load required libraries
library(tidyverse)
library(httr)
library(purrr)
library(readr)
library(stringr)
library(tibble)
library(kableExtra)
library(tidytext)
library(colorspace)
library(jsonlite)
library(lubridate)
```

## 1. Read and Prepare Raw JSON

In this step, we load the raw JSON file containing NYT articles. We first preserve its nested structure by setting `flatten = FALSE`. Because the JSON is deeply nested and large (\~596 entries), we avoid inspecting it directly and instead combine its elements into a single tibble using `map_dfr()`. Upon examining the structure, we notice that several fields, such as `creativeWorkHeadline`, are nested data.frames. To simplify analysis, we extract the `default` subfield (containing the main headline text) and unnest it into regular columns.

```{r}
#| label: step-1a

## =================================================================
# Step 1: Read and Prepare Raw JSON (flatten = FALSE)
# =================================================================
# Note: Setting flatten = FALSE is important here.
#       It preserves nested structure and prevents JSON explosion,
#       which allows for faster and more manageable parsing downstream.

# Load JSON scraped from Python
nyt_news <- fromJSON("../data/nyt_news_data.json", flatten = FALSE)

# We suggest to avoid running `glimpse(nyt_news)` directly because it is a very
# large list (~596 elements, deeply nested) and would result in an extremely
# extremely long and overwhelming chunk output.

# Instead, we first combine all the small data.frames inside `nyt_news`
# into a single large tibble for easier handling.

# Combine all the little data.frames into one big tibble
nyt_news_df <- map_dfr(nyt_news, as_tibble)

# View structure
glimpse(nyt_news_df)
```

```{r}
#| label: step-1b

# We observed that the `creativeWorkHeadline` column in `nyt_news_df`
# contains a nested data.frame with two subfields:
# - `__typename`: metadata about the headline type
# - `default`: the main headline text we want
#
# To simplify the dataset for analysis, we extract the `default` field
# as a new column. If missing, we assign NA.
# 
# This step "unnests" the nested structure into regular columns.

nyt_news_df <- nyt_news_df |>
  unnest_wider(creativeWorkHeadline, names_sep = "_")
```

## 2. Subset Metadata

In this step, we subset the New York Times dataset to retain only entries classified as articles. We then extract the relevant metadata, headline, publication date, URL, summary, and full text, and clean the publication date into a standard `Date` format for easier filtering later.

```{r}
#| label: step-2

# =================================================================
# Step 2: Subset Metadata
# =================================================================

nyt_meta <- nyt_news_df |>
  # Keep only entries classified as Article
  filter(`__typename` == "Article") |>
  transmute(
    title = creativeWorkHeadline_default,   # Article headline
    publicationDate = firstPublished,        # Publication timestamp
    url,                                     # Article URL
    description = creativeWorkSummary,       # Short summary
    text                                     # Full article body (if available)
  ) |>
  # Clean publication date format to Date class
  mutate(
    publicationDate = as.Date(publicationDate)
  )

```

## 3. Filter Articles by Publication Year (2020-2024)

In this step, we subset the dataset to include only articles published between 2020 and 2024. We first convert the `publicationDate` string into a proper datetime object using `lubridate::ymd_hms()`, then retain only those entries whose year falls within our desired range.

```{r}
#| label: step-3

# =================================================================
# Step 3: Filter Articles by Publication Year (2020-2024)
# =================================================================

nyt_meta2 <- nyt_meta |>
  # Parse publication date (DATE ONLY)
  mutate(pub_date = lubridate::ymd(publicationDate)) |>
  
  # Keep articles from 2020–2024
  filter(lubridate::year(pub_date) >= 2020,
         lubridate::year(pub_date) <= 2024) |>
  
  # Add Roe v. Wade overturning label
  mutate(roe_status = if_else(
    pub_date < as_date("2022-06-24"), 
    "Pre-Roe", 
    "Post-Roe"
  )) |>
  
  # Drop original unparsed column
  select(-publicationDate)
```

## 4. Clean Article Text

```{r}
#| label: step-4

# =================================================================
# Step 4: Clean Article Text
# =================================================================
nyt_meta3 <- nyt_meta2 |>
  # Remove rows with missing or blank text
  filter(!is.na(text), str_trim(text) != "") |>

  # Count abortion mentions (case-insensitive) and filter for ≥ 5
  mutate(abortion_mentions = str_count(str_to_lower(text), 
                                       "\\babortion\\b")) |>
  filter(abortion_mentions >= 5) |>

  # Minimal cleaning
  mutate(
    text_clean = text |> 
      str_replace_all("[\r\n\t]", " ") |>     # Normalize whitespace
      str_replace_all(" +", " ") |>           # Collapse multiple spaces
      str_trim()                              # Remove trailing/leading spaces
  )


write.csv(nyt_meta3, "../data/wrangled/nyt_news_data_wrangled.csv")

```

## 5. Text Analysis

```{r}
#| label: top-words

nyt_meta3 |>
  unnest_tokens(word, text_clean) |>
  filter(!word %in% stop_words$word, str_detect(word, "^[a-z]+$")) |>
  count(word, sort = TRUE) |>
  slice_max(n, n = 15) |>
  ggplot(aes(n, reorder(word, n))) +
  geom_col(fill = "steelblue") +
  labs(title = "Most Frequent Words", x = "Count", y = "")
```

```{r}
#| label: bigrams

nyt_meta3 |>
  unnest_tokens(bigram, text_clean, token = "ngrams", n = 2) |>
  separate(bigram, c("word1", "word2"), sep = " ") |>
  filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word) |>
  unite(bigram, word1, word2, sep = " ") |>
  count(bigram, sort = TRUE) |>
  slice_max(n, n = 15) |>
  ggplot(aes(n, reorder(bigram, n))) +
  geom_col(fill = "darkorange") +
  labs(title = "Top Bigrams", x = "Count", y = "")
```
```{r}
#| label: tf-idf

nyt_tf_idf <- nyt_meta3 |> 
  unnest_tokens(word, text_clean) |> 
  count(title, word, sort = TRUE) |> 
  bind_tf_idf(word, title, n)

top_tf_idf_words <- nyt_tf_idf |> 
  group_by(title) |> 
  slice_max(tf_idf, n = 5) |> 
  ungroup()

```


```{r}
#| label: afinn
afinn <- get_sentiments("afinn")

nyt_meta4 <- nyt_meta3 |> 
  unnest_tokens(word, text_clean) |> 
  left_join(afinn, by = "word")

sentiment_analyis_text <- nyt_meta4|> 
  group_by(title)|>
  filter(!is.na(value)) |> 
  summarise(text_sent_score = sum(value))



nyt_meta4 <- nyt_meta3 |> 
  unnest_tokens(word, description) |> 
  left_join(afinn, by = "word")

sentiment_analyis_description <- nyt_meta4|> 
  group_by(title)|>
  filter(!is.na(value)) |> 
  summarise(description_sent_score = sum(value))


nyt_meta5 <- nyt_meta3 |> 
  left_join(sentiment_analyis_text)|> 
  left_join(sentiment_analyis_description) |> 
  mutate(sentiment_score = 
           if_else(is.na(description_sent_score), 0, description_sent_score) + 
           if_else(is.na(text_sent_score), 0, text_sent_score))|> 
  select(-url, -description, -text)
```

```{r}
#| label: custom-stopwords
nyt_meta3 |>
  unnest_tokens(word, text_clean) |>
  count(word, sort = TRUE) |>
  filter(!word %in% stop_words$word) |>
  slice_max(n, n = 50)  # look at common terms that weren't removed

# Top frequent words not already removed
top_junk <- c("news", "said", "people", "mr", "times", "u.s", "including", "justice", "article", "new", "york")

# Bind to standard stop word list
custom_stop <- tibble(word = top_junk)
all_stops <- bind_rows(stop_words, custom_stop)

```






