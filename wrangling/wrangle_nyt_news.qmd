---
title: "Wrangling NBC News Articles"
subtitle: "STAT 231: Blog App"
author: "Maigan Lafontant, Emilie Ward, and Erika Salvador"
format: 
  pdf
header-includes: |
  \usepackage{fvextra}
  \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
---

```{r}
#| label: setup
#| include: false

# set code chunk option defaults
knitr::opts_chunk$set(
  # display code as types
  tidy = FALSE, 
  # slightly smaller code font
  size = "small",
  # do not display messages in PDF
  message = FALSE,
# improve digit and NA display 
  options(scipen = 1, knitr.kable.NA = ''))

# Load required libraries
library(tidyverse)
library(httr)
library(purrr)
library(readr)
library(stringr)
library(tibble)
library(kableExtra)
library(tidytext)
library(colorspace)
library(jsonlite)
library(lubridate)
```

## 1. Read and Prepare Raw JSON

In this step, we load the raw JSON file containing NYT articles. We first preserve its nested structure by setting `flatten = FALSE`. Because the JSON is deeply nested and large (\~596 entries), we avoid inspecting it directly and instead combine its elements into a single tibble using `map_dfr()`. Upon examining the structure, we notice that several fields, such as `creativeWorkHeadline`, are nested data.frames. To simplify analysis, we extract the `default` subfield (containing the main headline text) and unnest it into regular columns.

```{r}
#| label: step-1a

## =================================================================
# Step 1: Read and Prepare Raw JSON (flatten = FALSE)
# =================================================================
# Note: Setting flatten = FALSE is important here.
#       It preserves nested structure and prevents JSON explosion,
#       which allows for faster and more manageable parsing downstream.

# Load JSON scraped from Python
nyt_news <- fromJSON("../data/nyt_news_data.json", flatten = FALSE)

# We suggest to avoid running `glimpse(nyt_news)` directly because it is a very
# large list (~596 elements, deeply nested) and would result in an extremely
# extremely long and overwhelming chunk output.

# Instead, we first combine all the small data.frames inside `nyt_news`
# into a single large tibble for easier handling.

# Combine all the little data.frames into one big tibble
nyt_news_df <- map_dfr(nyt_news, as_tibble)

# View structure
glimpse(nyt_news_df)
```

```{r}
#| label: step-1b

# We observed that the `creativeWorkHeadline` column in `nyt_news_df`
# contains a nested data.frame with two subfields:
# - `__typename`: metadata about the headline type
# - `default`: the main headline text we want
#
# To simplify the dataset for analysis, we extract the `default` field
# as a new column. If missing, we assign NA.
# 
# This step "unnests" the nested structure into regular columns.

nyt_news_df <- nyt_news_df |>
  unnest_wider(creativeWorkHeadline, names_sep = "_")
```

## 2. Subset Metadata

In this step, we subset the New York Times dataset to retain only entries classified as articles. We then extract the relevant metadata, headline, publication date, URL, summary, and full text, and clean the publication date into a standard `Date` format for easier filtering later.

```{r}
#| label: step-2

# =================================================================
# Step 2: Subset Metadata
# =================================================================

nyt_meta <- nyt_news_df |>
  # Keep only entries classified as Article
  filter(`__typename` == "Article") |>
  transmute(
    title = creativeWorkHeadline_default,   # Article headline
    publicationDate = firstPublished,        # Publication timestamp
    url,                                     # Article URL
    description = creativeWorkSummary,       # Short summary
    text                                     # Full article body (if available)
  ) |>
  # Clean publication date format to Date class
  mutate(
    publicationDate = as.Date(publicationDate)
  )

```

## 3. Filter Articles by Publication Year (2020-2024)

In this step, we subset the dataset to include only articles published between 2020 and 2024. We first convert the `publicationDate` string into a proper datetime object using `lubridate::ymd_hms()`, then retain only those entries whose year falls within our desired range.

```{r}
#| label: step-3

# =================================================================
# Step 3: Filter Articles by Publication Year (2020-2024)
# =================================================================

nyt_meta2 <- nyt_meta |>
  # Parse publication date (DATE ONLY)
  mutate(pub_date = lubridate::ymd(publicationDate)) |>
  
  # Keep articles from 2020–2024
  filter(lubridate::year(pub_date) >= 2020,
         lubridate::year(pub_date) <= 2024) |>
  
  # Add Roe v. Wade overturning label
  mutate(roe_status = if_else(
    pub_date < as_date("2022-06-24"), 
    "Pre-Roe", 
    "Post-Roe"
  )) |>
  
  # Drop original unparsed column
  select(-publicationDate)
```

## 4. Clean Article Text

```{r}
#| label: step-4

# =================================================================
# Step 4: Clean Article Text
# =================================================================
nyt_meta3 <- nyt_meta2 |>
  # Remove rows with missing or blank text
  filter(!is.na(text), str_trim(text) != "") |>

  # Count abortion mentions (case-insensitive) and filter for ≥ 5
  mutate(abortion_mentions = str_count(str_to_lower(text), 
                                       "\\babortion\\b")) |>
  filter(abortion_mentions >= 5) |>

  # Minimal cleaning
  mutate(
    text_clean = text |> 
      str_replace_all("[\r\n\t]", " ") |>     # Normalize whitespace
      str_replace_all(" +", " ") |>           # Collapse multiple spaces
      str_trim()                              # Remove trailing/leading spaces
  )


write.csv(nyt_meta3, "../data/wrangled/nyt_news_data_wrangled.csv")

```

## 5. Text Analysis

```{r}
#| label: stop-words
data(stop_words)

top_junk <- c("news", "said", "people", "mr", "times", "u.s", "including", 
              "justice", "article", "new", "york", "it’s", "don’t", 
              "abortions", "week", "that's", "ms", "supreme", "court", "abortion")

custom_stop <- tibble(word = top_junk)
all_stops <- bind_rows(stop_words, custom_stop)
```


```{r}
#| label: top-words
nyt_meta3 |> 
  unnest_tokens(word, text_clean) |>
  filter(!word %in% all_stops$word, str_detect(word, "^[a-z]+$")) |>
  count(word, sort = TRUE) |>
  slice_max(n, n = 15) |>
  ggplot(aes(n, reorder(word, n))) +
  geom_col(fill = "steelblue") +
  labs(title = "Most Frequent Words", x = "Count", y = "")

```


```{r}
#| label: top-bigrams

nyt_meta3 |> 
  unnest_tokens(bigram, text_clean, token = "ngrams", n = 2) |>
  separate(bigram, c("word1", "word2"), sep = " ") |>
  filter(!word1 %in% all_stops$word, !word2 %in% all_stops$word) |>
  unite(bigram, word1, word2, sep = " ") |>
  count(bigram, sort = TRUE) |>
  slice_max(n, n = 15) |>
  ggplot(aes(n, reorder(bigram, n))) +
  geom_col(fill = "darkorange") +
  labs(title = "Top Bigrams", x = "Count", y = "")

```


```{r}
#| label: afinn

afinn <- get_sentiments("afinn")

# Tokenize and filter
nyt_words <- nyt_meta3 |> 
  unnest_tokens(word, text_clean) |>
  filter(!word %in% all_stops$word)

# Join with AFINN scores
nyt_sentiment <- nyt_words |> 
  left_join(afinn, by = "word")

# Summarize sentiment per article
article_sentiment <- nyt_sentiment |> 
  group_by(title, roe_status, pub_date) |> 
  summarise(sentiment_score = sum(value, na.rm = TRUE), .groups = "drop")

```

## Step 6: Visualizations

```{r}
#| label: sentiment-distribution

article_sentiment |> 
  ggplot(aes(x = sentiment_score)) +
  geom_histogram(binwidth = 5, fill = "skyblue") +
  labs(title = "Distribution of NYT Article Sentiment Scores", 
       x = "Sentiment Score", y = "Number of Articles")

```

```{r}
#| label: sentiment-by-roe

ggplot(article_sentiment, aes(x = title, y = sentiment_score, fill = roe_status)) +
  geom_bar(stat = "identity") +
  labs(title = "New York Times Articles Sentiment by Roe Status", 
       x = "", y = "Sentiment Score", fill = "Pre & Post Roe") +
  facet_grid(~roe_status) +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())

```


```{r}
#| label: sentiment-over-time

article_sentiment_time <- article_sentiment |>
  group_by(date = floor_date(pub_date, "month")) |>
  summarise(mean_sentiment = mean(sentiment_score, na.rm = TRUE))

ggplot(article_sentiment_time, aes(x = date, y = mean_sentiment)) +
  geom_line(color = "#003f5c") +
  geom_smooth(method = "loess", se = FALSE, color = "black") +
  labs(title = "Average NYT Article Sentiment Over Time", x = "Month", y = "Mean Sentiment Score")

```


```{r}
#| label: roe-articles

roe_articles <- nyt_meta3 |> 
  filter(str_detect(text_clean, "roe|wade|supreme|court|overturn|decision")) |>
  mutate(topic = "Roe v. Wade")

roe_sentiment <- roe_articles |> 
  unnest_tokens(word, text_clean) |> 
  filter(!word %in% all_stops$word) |> 
  left_join(afinn, by = "word") |> 
  group_by(title, pub_date) |> 
  summarise(sentiment_score = sum(value, na.rm = TRUE), .groups = "drop")

roe_sentiment_monthly <- roe_sentiment |> 
  mutate(month = floor_date(pub_date, "month")) |> 
  group_by(month) |> 
  summarise(mean_sentiment = mean(sentiment_score, na.rm = TRUE))

ggplot(roe_sentiment_monthly, aes(x = month, y = mean_sentiment)) +
  geom_line(color = "#003f5c") +
  geom_point(size = 2) +
    geom_vline(xintercept = as.Date("2022-06-24"), 
             color = "red", linetype = "dashed", size = 1) +
    annotate("text", x = as.Date("2022-06-24"), 
           y = max(roe_sentiment_monthly$mean_sentiment, na.rm = TRUE) + 5,
           label = "Dobbs Decision", color = "black", angle = 0, vjust = 1.0, hjust = -0.1) +
   geom_vline(xintercept = as.Date("2021-11-01"), 
             color = "red", linetype = "dashed", size = 1) +
  annotate("text", x = as.Date("2021-11-01"), y = -40, label = "Dobbs arguments heard\nby Supreme Court",
         color = "black", hjust = 1.1,  vjust = 1.0) +
  labs(title = "NYT Sentiment Around Roe v. Wade Overturning", 
       x = "Year", y = "Average Sentiment Score") +
  theme_minimal()


```


```{r}
#| label: wordcloud-by-sentiment-polarity

library(wordcloud2)

#negative sentiment words
nyt_sentiment |> 
  filter(value < 0) |>
  count(word, sort = TRUE) |>
  slice_max(n, n = 50) |>
wordcloud2(size = 0.7, color = "random-dark", backgroundColor = "white")

#positive sentiment words 
nyt_sentiment |> 
  filter(value > 0) |>
  count(word, sort = TRUE) |>
  slice_max(n, n = 50) |>
wordcloud2(size = 0.7, color = "random-dark", backgroundColor = "white")

```

```{r}
#| label: top-sentiment-words
nyt_sentiment |>
  count(word, value, sort = TRUE) |>
  filter(!is.na(value)) |>
  group_by(sign = ifelse(value > 0, "positive", "negative")) |>
  slice_max(n, n = 10) |>
  ungroup() |>
  ggplot(aes(x = n, y = reorder(word, n), fill = sign)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sign, scales = "free_y") +
  labs(title = "Top Sentiment Words in NYT Coverage", x = "Count", y = "Word")

```














