---
title: "Wrangling Fox News Articles"
subtitle: "STAT 231: Blog App"
author: "Maigan Lafontant, Emilie Ward, and Erika Salvador"
format: 
  pdf
header-includes: |
  \usepackage{fvextra}
  \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
---

```{r}
#| label: setup
#| include: false

# set code chunk option defaults
knitr::opts_chunk$set(
  # display code as types
  tidy = FALSE, 
  # slightly smaller code font
  size = "small",
  # do not display messages in PDF
  message = FALSE,
# improve digit and NA display 
  options(scipen = 1, knitr.kable.NA = ''))

# Load required libraries
library(tidyverse)
library(httr)
library(readr)
library(stringr)
library(tibble)
library(kableExtra)
library(colorspace)
library(jsonlite)
library(lubridate)
```

## 1. Read and Prepare Raw JSON

The JSON file `fox_news_data.json` was scraped from Fox News using a Python-based Tor-enabled web scraper. It includes article-level information such as titles, dates, body text, links, and images.

```{r}
#| label: step-1

# =================================================================
# Step 1: Read and Prepare Raw JSON (flatten = FALSE)
# =================================================================
# Note: Setting flatten = FALSE is important here.
#       It preserves nested structure and prevents JSON explosion,
#       which allows for faster and more manageable parsing downstream.

# Load JSON scraped from Python
fox_news <- fromJSON("../data/fox_news_data.json", flatten = FALSE)

# Preview structure
glimpse(fox_news)
```

## 2. Subset Metadata

In this step, we subset and clean the raw Fox News data to retain only the important metadata fields needed for our analysis. Specifically, we extract the article title, description, full text, publication date, URL, and author information. The `publicationDate` field is simplified by selecting only the first timestamp per article. Because the original `url` values are relative paths, we prepend the full domain (`https://www.foxnews.com`) to make them complete. The `authors` column is nested and occasionally malformed, so we use safety checks to extract and collapse the authors' names into a single string or return `NA` if no valid data is found.

```{r}
#| label: step-2

# =================================================================
# Step 2: Subset Metadata
# =================================================================

fox_meta <- fox_news |>
  # Convert to tibble for easier manipulation
  as_tibble() |>
  transmute(
    title,  # Article headline

    authors = map_chr(authors, ~ {  # Safely extract author names
      if (is.null(.x) || !is.data.frame(.x) || ncol(.x) == 0) {
        NA_character_
      } else {
        paste(unlist(.x), collapse = ", ")
      }
    }),
    
    # Use the first timestamp
    publicationDate = map_chr(publicationDate, ~ .x[1]),  
    # Convert relative to absolute URLs
    url = paste0("https://www.foxnews.com", url),         
    description,  # Short summary
    text          # Full article body
  )
```

## 3. Filter Articles by Publication Year (2020-2024)

In this step, we subset the dataset to include only articles published between 2020 and 2024. We first convert the `publicationDate` string into a proper datetime object using `lubridate::ymd_hms()`, then retain only those entries whose year falls within our desired range.

```{r}
#| label: step-3

# =================================================================
# Step 3: Filter Articles by Publication Year (2020-2024)
# =================================================================

# Filter to articles published between 2020 and 2024
fox_meta2 <- fox_meta |> 
  
  # Parse publication date
  mutate(pub_date = lubridate::ymd_hms(publicationDate)) |>  
  
  # Keep 2020-2024
  filter(lubridate::year(pub_date) >= 2020, 
         lubridate::year(pub_date) <= 2024) |> 
  
  # Add Roe v. Wade overturning label
  mutate(roe_status = if_else(pub_date < as_datetime("2022-06-24"), "
                              Pre-Roe", "Post-Roe")) |> 
  
  # Drop original unparsed column
  select(-publicationDate)  
```

## 4. Clean Article Text

To prepare the dataset for text analysis, we implemented a custom cleaning function that systematically removes non-informative and repetitive patterns from the article body. These include promotional banners (e.g., “Fox News Flash headlines” and “Click here to get the FOX NEWS app”), social media prompts, author bylines, timestamps, and sections like “Related Topics” or “More from Politics” that often appear at the end of articles. We also stripped out visual clutter and interface references such as “Read more” links, multimedia mentions (e.g., “video” or “photo”), and special characters like curly quotes, em dashes, non-breaking spaces, and excess whitespace.

That said, this is unfortunately just a preliminary cleaning step. Despite our efforts to regularize the text in R, the original formatting was too inconsistent and fragmented for full automation. As a result, we had to export the cleaned dataset and manually correct remaining irregularities in Google Sheets as a team.

```{r}
#| label: step 4a

# =================================================================
# Step 4: Clean Article Text
# =================================================================

clean_text_column <- function(text) {
  text |>
    # Remove promotional section headers 
    str_remove_all("(?i)Fox News Flash.*?headlines") |>
    
    # Remove call-to-action lines 
    str_remove_all("(?i)CLICK HERE TO GET THE FOX NEWS APP") |>
    
    # Remove newsletter subscription prompts
    str_remove_all("(?i)Subscribe.*?newsletter") |>
    
    # Remove confirmation blurbs like 
    str_remove_all("(?i)You(’|')ve successfully subscribed.*?newsletter") |>
    
    # Remove social media buttons and UI text
    str_remove_all("(?i)Facebook Twitter Flipboard Comments Print Email") |>
    
    # Remove stray "close" buttons or labels from modal windows
    str_remove_all("(?i)close") |>
    
    # Remove "Related Topics" blurbs typically found at the end of articles
    str_remove_all("(?i)Related Topics.*?(?=\\b[A-Z])") |>
    
    # Remove "More from [section]" banners (e.g., More from Politics)
    str_remove_all("(?i)More from [A-Z][a-z]+") |>
    
    # Remove "Read more" links or references
    str_remove_all("(?i)Read more") |>
    
    # Remove author bylines like "By John Doe Fox News"
    str_remove_all("(?i)By\\s+[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\s+Fox News") |>
    
    # Remove publication dates 
    str_remove_all("(?i)Published\\s+[A-Z][a-z]+\\s+\\d{1,2},\\s+\\d{4}.*?(AM|PM|EST|EDT)?") |>
    
    # Remove embedded media labels like video, photo, image, etc.
    str_remove_all("(?i)video|photo|image|email|print") |>
    
    # Normalize curly single quotes to straight quotes
    str_replace_all("‘|’", "'") |>
    
    # Normalize curly double quotes to straight quotes
    str_replace_all("“|”", "\"") |>
    
    # Normalize long/em dashes to regular dashes
    str_replace_all("–|—", "-") |>
    
    # Normalize ellipses to three dots
    str_replace_all("…", "...") |>
    
    # Replace non-breaking spaces with regular spaces
    str_replace_all("\u00A0", " ") |>
    
    # Collapse multiple spaces/tabs/newlines into a single space
    str_replace_all("\\s+", " ") |>
    
    # Remove any leftover bracketed content (e.g., [Advertisement])
    str_remove_all("\\[.*?\\]") |>
    
    # Remove any leftover brace-enclosed artifacts
    str_remove_all("\\{.*?\\}") |>
    
    # Trim leading and trailing whitespace
    str_trim()
}

# Apply the cleaning function to the filtered dataset
fox_meta3 <- fox_meta2 |> 
  mutate(
    # Create new column with cleaned text
    text_cleaned = clean_text_column(text)  
  ) 

# Save the cleaned version to CSV for further manual correction
write_csv(
  fox_meta3,
  "fox_meta_raw_texts.csv"
)
```
