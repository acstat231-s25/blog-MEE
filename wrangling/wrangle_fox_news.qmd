---
title: "Wrangling Fox News Articles"
subtitle: "STAT 231: Blog App"
author: "Maigan Lafontant, Emilie Ward, and Erika Salvador"
format: 
  pdf
header-includes: |
  \usepackage{fvextra}
  \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
---

```{r}
#| label: setup
#| include: false

# set code chunk option defaults
knitr::opts_chunk$set(
  # display code as types
  tidy = FALSE, 
  # slightly smaller code font
  size = "small",
  # do not display messages in PDF
  message = FALSE,
# improve digit and NA display 
  options(scipen = 1, knitr.kable.NA = ''))

# Load required libraries
library(tidyverse)
library(httr)
library(readr)
library(stringr)
library(tibble)
library(kableExtra)
library(colorspace)
library(jsonlite)
library(tidytext)
library(lubridate)
```

## 1. Read and Prepare Raw JSON

The JSON file `fox_news_data.json` was scraped from Fox News using a Python-based Tor-enabled web scraper. It includes article-level information such as titles, dates, body text, links, and images.

```{r}
#| label: step-1

# =================================================================
# Step 1: Read and Prepare Raw JSON (flatten = FALSE)
# =================================================================
# Note: Setting flatten = FALSE is important here.
#       It preserves nested structure and prevents JSON explosion,
#       which allows for faster and more manageable parsing downstream.

# Load JSON scraped from Python
fox_news <- fromJSON("../data/fox_news_data.json", flatten = FALSE)

# Preview structure
glimpse(fox_news)
```

## 2. Subset Metadata

In this step, we subset and clean the raw Fox News data to retain only the important metadata fields needed for our analysis. Specifically, we extract the article title, description, full text, publication date, and URL. The `publicationDate` field is simplified by selecting only the first timestamp per article. Because the original `url` values are relative paths, we prepend the full domain (`https://www.foxnews.com`) to make them complete. The `authors` column is nested and occasionally malformed, so we use safety checks to extract and collapse the authors' names into a single string or return `NA` if no valid data is found.

```{r}
#| label: step-2

# =================================================================
# Step 2: Subset Metadata
# =================================================================

fox_meta <- fox_news |>
  # Convert to tibble for easier manipulation
  as_tibble() |>
  transmute(
    title,  # Article headline
    # Use the first timestamp
    publicationDate = map_chr(publicationDate, ~ .x[1]),  
    # Convert relative to absolute URLs
    url = paste0("https://www.foxnews.com", url),         
    description,  # Short summary
    text          # Full article body
  )
```

## 3. Filter Articles by Publication Year (2020-2024)

In this step, we subset the dataset to include only articles published between 2020 and 2024. We first convert the `publicationDate` string into a proper datetime object using `lubridate::ymd_hms()`, then retain only those entries whose year falls within our desired range.

```{r}
#| label: step-3

# =================================================================
# Step 3: Filter Articles by Publication Year (2020-2024)
# =================================================================

# Filter to articles published between 2020 and 2024
fox_meta2 <- fox_meta |> 
  
  # Parse publication date
  mutate(pub_date = lubridate::ymd_hms(publicationDate)) |>  
  
  # Keep 2020-2024
  filter(lubridate::year(pub_date) >= 2020, 
         lubridate::year(pub_date) <= 2024) |> 
  
  # Add Roe v. Wade overturning label
  mutate(roe_status = if_else(pub_date < as_datetime("2022-06-24"), "
                              Pre-Roe", "Post-Roe")) |> 
  
  # Drop original unparsed column
  select(-publicationDate)  
```

## 4. Clean Article Text

Our first trial of scraping returned a cluttered dataset filled with interface fragments such as “Subscribe Now” prompts, author bylines like “Written by,” and unrelated navigation blocks. Many entries pointed to videos or other media content that had no accompanying article text. After reviewing the structure of the site, we found the correct CSS selector for isolating full article bodies. This allowed us to extract only the article content and avoid the noise embedded in other page elements. At this point, we only needed to remove articles with missing or empty text fields, most of which corresponded to media-only items such as videos or photo galleries tagged with “abortion” but without any written content. We also limited our dataset to articles with at least five mentions of the word “abortion” to ensure that each piece contributed meaningfully to the topic.

```{r}
#| label: step-4

# =================================================================
# Step 4: Clean and Filter Article Text
# =================================================================

fox_meta3 <- fox_meta2 |>
  # Remove rows with missing or blank text
  filter(!is.na(text), str_trim(text) != "") |>

  # Count abortion mentions (case-insensitive) and filter for ≥ 5
  mutate(abortion_mentions = str_count(str_to_lower(text), 
                                       "\\babortion\\b")) |>
  filter(abortion_mentions >= 5) |>

  # Minimal cleaning
  mutate(
    text_clean = text |> 
      str_replace_all("[\r\n\t]", " ") |>     # Normalize whitespace
      str_replace_all(" +", " ") |>           # Collapse multiple spaces
      str_trim()                              # Remove trailing/leading spaces
  )

write_csv(fox_meta3, "../data/wrangled/fox_news_data_wrangled.csv")
```


## 5. Text Analysis

```{r}
#| label: top-words

fox_meta3 |>
  unnest_tokens(word, text_clean) |>
  filter(!word %in% stop_words$word, str_detect(word, "^[a-z]+$")) |>
  count(word, sort = TRUE) |>
  slice_max(n, n = 15) |>
  ggplot(aes(n, reorder(word, n))) +
  geom_col(fill = "steelblue") +
  labs(title = "Most Frequent Words", x = "Count", y = "")
```

```{r}
#| label: bigrams

fox_meta3 |>
  unnest_tokens(bigram, text_clean, token = "ngrams", n = 2) |>
  separate(bigram, c("word1", "word2"), sep = " ") |>
  filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word) |>
  unite(bigram, word1, word2, sep = " ") |>
  count(bigram, sort = TRUE) |>
  slice_max(n, n = 15) |>
  ggplot(aes(n, reorder(bigram, n))) +
  geom_col(fill = "darkorange") +
  labs(title = "Top Bigrams", x = "Count", y = "")
```

```{r}
#| label: afinn
afinn <- get_sentiments("afinn")

#This analysis looks through the text for more specific tags such as religious, medical, gender, and legal tags 
top_junk <- c("news", "said", "people", "mr", "fox", "u.s", "including", "justice", "a", "the", "abortion", "supreme", "court", "it's")
health <- c("health", "clinic", "medicine", "pills", "planned parenthood")
gender <- c("female", "females", "women", "woman", "girl", "girls")
legal <- c("illegal", "legal", "court", "ruling", "law")

fox_text_content <- fox_meta3 |> 
  unnest_tokens(sentence, text, token = "sentences")|> 
  mutate(terms = if_else(str_detect(sentence, str_c(health, collapse = "|")), 
                           "Health", NA)) |>
  mutate(terms = if_else(
    str_detect(sentence, str_c(gender, collapse = "|")), if_else(is.na(terms), "Gender", 
                                   paste(terms, "Gender", sep = ", ")), terms))|> 
  mutate(terms = if_else(
    str_detect(sentence, str_c(legal, collapse = "|")), if_else(is.na(terms), "Legal", 
                                   paste(terms, "Legal", sep = ", ")), terms))
  
fox_text_content <- fox_text_content|> 
  filter(!is.na(terms))

fox_text_content |> 
  group_by(terms) |> 
  summarise(occurance = n()) |>
  kable()

```

In this next part of the key word analysis we will take a look the words in the sentences with these tags to understand what are the general topics and points of conversation around these keywords. 

```{r}
word_freq <- fox_text_content |>
  unnest_tokens(word, sentence) |> 
  anti_join(stop_words) 

set.seed(53)

png("/Users/emilieward/Desktop/gendercloud.png", width = 500, height = 500)
word_freq_gender <- word_freq |> 
  filter(str_detect(terms, "Gender")) |> 
  filter(!str_detect(word, str_c(top_junk, collapse =  "|"))) |> 
  filter(!str_detect(word, str_c(gender, collapse =  "|")))|> 
  count(word, sort = TRUE) |> 
  with(wordcloud(words = word, freq = n, max.words = 50))
dev.off()

png("/Users/emilieward/Desktop/healthcloud.png", width = 500, height = 500)
word_freq_health <- word_freq |> 
  filter(str_detect(terms, "Health")) |> 
  filter(!str_detect(word, str_c(top_junk, collapse =  "|")))|> 
  filter(!str_detect(word, str_c(health, collapse =  "|")))|> 
  count(word, sort = TRUE) |> 
  with(wordcloud(words = word, freq = n, max.words = 50))
dev.off()

png("/Users/emilieward/Desktop/legalcloud.png", width = 500, height = 500)
word_freq_law <- word_freq |> 
  filter(str_detect(terms, "Legal")) |> 
  filter(!str_detect(word, str_c(top_junk, collapse =  "|")))|> 
  filter(!str_detect(word, str_c(legal, collapse =  "|")))|> 
  count(word, sort = TRUE) |> 
  with(wordcloud(words = word, freq = n, max.words = 50))
dev.off()

```

## 6. Bigrams

In this sections we begin to explore the different relationship and trends through figures. We isolate and consolidate different factors to see different trends, from the mention of keywords to the date that it was published

```{r}
#| label: custom-stopwords
fox_meta3 |>
  unnest_tokens(word, text_clean) |>
  count(word, sort = TRUE) |>
  filter(!word %in% stop_words$word) |>
  slice_max(n, n = 50)  # look at common terms that weren't removed

# Top frequent words not already removed
top_junk <- c("news", "said", "people", "mr", "fox", "u.s", "including", "justice", "a", "the", "abortion", "supreme", "court")

# Bind to standard stop word list
custom_stop <- tibble(word = top_junk)
all_stops <- bind_rows(stop_words, custom_stop)

fox_meta_topwords <- fox_meta3 |>
  select(text, title) |> 
  unnest_tokens(word, text, token = "ngrams", n= 1) |> 
  filter(!word %in% top_junk) |> 
  filter(!word %in% all_stops$word) |> 
  filter(!str_detect(word, "\\d+")) |>
  group_by(title) |> 
  summarize(text_cleaned = paste(word, collapse = " "),.groups = "drop") 
  
# g_topwords <- fox_meta_topwords |> 
#   unnest_tokens(word, text_cleaned, token = "ngrams", n= 1) |>
#   filter(word != "life" | word != "wade") |> 
#   count(word, sort = TRUE) |>
#   slice_max(n, n = 15) |>
#   ggplot(aes(n, reorder(word, n))) +
#   geom_col(fill = "steelblue") +
#   labs(title = "Most Frequent Words", x = "Count", y = "")
# g_topwords


g_topbigram <- fox_meta_topwords |>
  unnest_tokens(bigram, text_cleaned, token = "ngrams", n = 2) |>
  count(bigram, sort = TRUE) |>
  slice_max(n, n = 15) |>
  ggplot(aes(n, reorder(bigram, n))) +
  geom_col(fill = "darkorange") +
  labs(title = "Top Bigrams", x = "Count", y = "")
g_topbigram

ggsave("/Users/emilieward/Desktop/bigrams.png", plot = g_topbigram, width = 8, height = 6)
```


```

